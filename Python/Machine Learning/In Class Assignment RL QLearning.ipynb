{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cc4199",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "<img align=\"left\" src=\"image1.png\"     style=\" width:1000px; padding: 10px; \" >\n",
    "\n",
    "- Reinforcement learning is a subfield of machine learning that focuses on developing algorithms that enable an agent to learn how to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties, and its goal is to learn how to take actions that maximize its cumulative rewards over time.\n",
    "\n",
    "\n",
    "- In reinforcement learning, the agent interacts with the environment in discrete time steps. At each time step, the agent observes the current state of the environment and selects an action to take based on a policy, which is a function that maps states to actions. The environment then transitions to a new state and provides the agent with a reward signal, which is a scalar value that indicates how well the agent is doing at the current time step. The agent then updates its policy based on the observed state, action, and reward, and continues to interact with the environment.\n",
    "\n",
    "\n",
    "- The goal of the agent is to learn a policy that maximizes its expected cumulative reward over time. This requires balancing the desire to maximize immediate rewards with the need to explore new actions that may lead to higher long-term rewards. Reinforcement learning algorithms use various techniques to balance exploration and exploitation and learn an optimal policy, such as Q-learning, policy gradient methods, and actor-critic methods.\n",
    "\n",
    "\n",
    "- Reinforcement learning has many practical applications, including robotics, game playing, recommendation systems, and autonomous vehicles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937556e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e127982",
   "metadata": {},
   "source": [
    "- Agent: Learns from trial & error\n",
    "- Environment: Where the agents moves\n",
    "- Action: All possible steps agent can take\n",
    "- State: Current condition in the environment\n",
    "- Reward: An apraisal of the last action\n",
    "- Policy: Agent uses to determine the next action based on the current state\n",
    "- Value: Expected long term return, as opposed to short term reward\n",
    "- Action-value: Similar to value, but it takes an extra parameter, the current action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13bbbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72d5eaad",
   "metadata": {},
   "source": [
    "## Markov's Decision Process\n",
    "\n",
    "<img align=\"left\" src=\"image2.png\"     style=\" width:400px; padding: 10px; \" >\n",
    "\n",
    "- The mathematical approach for mapping a solution in reinforcement learning is called MDP\n",
    "- Following parameters are used in attaining a solution in MDP:\n",
    "    - Set of actions, A\n",
    "    - Set of states, S\n",
    "    - Reward, R\n",
    "    - Policy, pi\n",
    "    - Value, V\n",
    " \n",
    "- Goal: Find the shortest path between A & D with minimum possible cost\n",
    "    - Set of states are denoted by nodes\n",
    "    - Action are denoted by edges\n",
    "    - Reward is the cost of an edge\n",
    "    - Policy is the path take to reach the destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e663f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0926ab1b",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "- Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions]\n",
    "- The Gamma parameter has a range of 0 to 1 (0 <= Gamma < 1)\n",
    "  - if Gamma is set closer to zero, agent will tend to consider only immediate rewards - exploitation\n",
    "  - if Gamma is set closer to one, agent will consider higher rewards - exploration\n",
    "  \n",
    "<img align=\"right\" src=\"image4.png\"     style=\" width:300px; padding: 10px; \" >\n",
    "<img align=\"left\" src=\"image3.png\"     style=\" width:300px; padding: 10px; \" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80321db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f63d83b4",
   "metadata": {},
   "source": [
    "## Q-Learning Example\n",
    "- Set gamma = 0.8\n",
    "- Initialize Q Matrix as zero\n",
    "- From node 1, agent can either go to node 3 or 5; let's select 5\n",
    "- From node 5, calculate maximum Q value for this next state based on potential actions\n",
    "- Q(state, action) = R(state, action) + gamma * max[Q(next state, all actions)]\n",
    "\n",
    "<img align=\"left\" src=\"image5.png\"     style=\" width:1000px; padding: 10px; \" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e992e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966e5c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.matrix(np.zeros([6,6]))\n",
    "initial_state = 1\n",
    "gamma = 0.8\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06e4cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -1,  -1,  -1,  -1,   0,  -1],\n",
       "        [ -1,  -1,  -1,   0,  -1, 100],\n",
       "        [ -1,  -1,  -1,   0,  -1,  -1],\n",
       "        [ -1,   0,   0,  -1,   0,  -1],\n",
       "        [  0,  -1,  -1,   0,  -1, 100],\n",
       "        [ -1,   0,  -1,  -1,   0, 100]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-1 â†’ NULL\n",
    "R = np.matrix([\n",
    "    [-1, -1, -1, -1,  0,  -1],\n",
    "    [-1, -1, -1,  0, -1, 100],\n",
    "    [-1, -1, -1,  0, -1,  -1],\n",
    "    [-1,  0,  0, -1,  0,  -1],\n",
    "    [ 0, -1, -1,  0, -1, 100],\n",
    "    [-1,  0, -1, -1,  0, 100]\n",
    "])\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84730536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1 -1  0 -1 -1]]\n",
      "(array([0, 0, 0, 0, 0, 0], dtype=int64), array([0, 1, 2, 3, 4, 5], dtype=int64))\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "#practice statements \n",
    "print(R[2,])\n",
    "current_state_row = R[2,]\n",
    "print(np.where(current_state_row >= -1))\n",
    "print(np.where(current_state_row >= -1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15faddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state):\n",
    "    current_state_row = R[state,]\n",
    "    avail_act = np.where(current_state_row >= 0)[1]\n",
    "    return avail_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85b091ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_act = available_actions(initial_state)\n",
    "available_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "507eabca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_next_action(available_actions_range):\n",
    "    next_action = int(np.random.choice(available_actions_range))\n",
    "    return next_action\n",
    "\n",
    "action = sample_next_action(available_act)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1be4f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  5\n",
      "max_index 1: (array([0, 0, 0, 0, 0, 0], dtype=int64), array([0, 1, 2, 3, 4, 5], dtype=int64))\n",
      "max_index 2: [0 1 2 3 4 5]\n",
      "max_index.shape (6,)\n",
      "random_max_index 1: 1\n",
      "random_max_index 2: 4\n",
      "\n",
      "\n",
      "R[action, max_index]: [[ -1   0  -1  -1   0 100]]\n",
      "current_state: 1\n",
      "R[current_state, action]: 100\n",
      "Q_learning: [[100. 100. 100. 100. 100. 100.]]\n"
     ]
    }
   ],
   "source": [
    "# Running various commands from the update, sample_next_action functions \n",
    "# to learn the interworkings of these commands\n",
    "\n",
    "action = sample_next_action(available_act)\n",
    "print(f'action:  {action}')\n",
    "max_index = np.where(Q[3, ] == np.max(Q[3, ]))\n",
    "print(f'max_index 1: {max_index}')\n",
    "max_index = np.where(Q[3, ] == np.max(Q[3, ]))[1]\n",
    "print(f'max_index 2: {max_index}')\n",
    "print(f'max_index.shape {max_index.shape}')\n",
    "random_max_index = int(np.random.choice(max_index, size=1))\n",
    "print(f'random_max_index 1: {random_max_index}')\n",
    "random_max_index = int(np.random.choice(max_index, size=1)[0])\n",
    "print(f'random_max_index 2: {random_max_index}')\n",
    "print(f'\\n')\n",
    "print(f'R[action, max_index]: {R[action, max_index]}')\n",
    "current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "print(f'current_state: {current_state}')\n",
    "max_value = Q[action, max_index]\n",
    "print(f'R[current_state, action]: {R[current_state, action]}')\n",
    "Q_learning = R[current_state, action] + gamma*max_value\n",
    "print(f'Q_learning: {Q_learning}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f651d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(current_state, action, gamma):\n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
    "    \n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index, size=1)[0])\n",
    "    else:\n",
    "        max_index = int(max_index.item())\n",
    "        \n",
    "    max_value = Q[action, max_index]\n",
    "    \n",
    "    Q[current_state, action] = R[current_state, action] + gamma * max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bc4ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Q Matrix\n",
      "[[  0.    0.    0.    0.   80.    0. ]\n",
      " [  0.    0.    0.   64.    0.  100. ]\n",
      " [  0.    0.    0.   64.    0.    0. ]\n",
      " [  0.   80.   51.2   0.   80.    0. ]\n",
      " [ 64.    0.    0.   64.    0.  100. ]\n",
      " [  0.   80.    0.    0.   80.  100. ]]\n"
     ]
    }
   ],
   "source": [
    "# Training the model to create the Q Matrix \n",
    "for i in range(10000):\n",
    "    current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "    available_act = available_actions(current_state)\n",
    "    action = sample_next_action(available_act)\n",
    "    update(current_state, action, gamma)\n",
    "    \n",
    "print('Train Q Matrix')\n",
    "print(Q / np.max(Q)* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45086c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "Selected Path: \n",
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "current_state = 2\n",
    "steps = [current_state]\n",
    "print(steps)\n",
    "while current_state != 5:\n",
    "    next_step_index = np.where(Q[current_state, ] == np.max(Q[current_state, ]))[1]\n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size=1)[0])\n",
    "    else:\n",
    "        next_step_index = int(next_step_index[0])\n",
    "    \n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index\n",
    "    \n",
    "print('Selected Path: ')\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d289482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
